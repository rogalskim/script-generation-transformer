{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "===============================================================\n",
    "\n",
    "This is a tutorial on how to train a sequence-to-sequence model\n",
    "that uses the\n",
    "`nn.Transformer <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer>`__ module.\n",
    "\n",
    "PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper `Attention is All You\n",
    "Need <https://arxiv.org/pdf/1706.03762.pdf>`__. The transformer model\n",
    "has been proved to be superior in quality for many sequence-to-sequence\n",
    "problems while being more parallelizable. The ``nn.Transformer`` module\n",
    "relies entirely on an attention mechanism (another module recently\n",
    "implemented as `nn.MultiheadAttention <https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention>`__) to draw global dependencies\n",
    "between input and output. The ``nn.Transformer`` module is now highly\n",
    "modularized such that a single component (like `nn.TransformerEncoder <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>`__\n",
    "in this tutorial) can be easily adapted/composed.\n",
    "\n",
    "![](../_static/img/transformer_architecture.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we train ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "`nn.TransformerEncoderLayer <https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer>`__. Along with the input sequence, a square\n",
    "attention mask is required because the self-attention layers in\n",
    "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "the sequence. For the language modeling task, any tokens on the future\n",
    "positions should be masked. To have the actual words, the output\n",
    "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "layer, which is followed by a log-Softmax function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dimension: int, dropout: int=0.1, max_length: int=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        positional_values = self.__generate_position_values(dimension, max_length)\n",
    "        self.register_buffer(\"positional_values\", positional_values)\n",
    "        \n",
    "    @staticmethod\n",
    "    def __generate_position_values(dimension: int, max_length: int):\n",
    "        values = torch.zeros(max_length, dimension)\n",
    "        positions = torch.arange(0, max_length, dtype=torch.float)\n",
    "        positions = positions.unsqueeze(1)\n",
    "        \n",
    "        scaling_steps = torch.arange(0, dimension, 2).float()\n",
    "        scaling = torch.exp(scaling_steps * (-math.log(10000.0)/dimension))\n",
    "        \n",
    "        values[:, 0::2] = torch.sin(positions * scaling)\n",
    "        values[:, 1::2] = torch.cos(positions * scaling)\n",
    "        values = values.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        return values\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_values[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerScriptGenerator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocabulary_size: int,\n",
    "                 embedding_dim: int,\n",
    "                 attention_head_count: int,\n",
    "                 encoder_fc_dim: int,\n",
    "                 encoder_layer_count: int,\n",
    "                 dropout: float=0.5) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_mask = None\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        self.embedding_scale = math.sqrt(embedding_dim)\n",
    "        self.positional_encoder = PositionalEncoding(embedding_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, attention_head_count, encoder_fc_dim, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, encoder_layer_count)\n",
    "        self.decoder = nn.Linear(embedding_dim, vocabulary_size)\n",
    "        \n",
    "        self.__init_weights()\n",
    "        \n",
    "    def __init_weights(self) -> None:\n",
    "        value_range = 0.1\n",
    "        self.embedding.weight.data.uniform_(-value_range, value_range)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-value_range, value_range)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def __generate_input_mask(mask_size: int) -> torch.Tensor():\n",
    "        mask = torch.ones(mask_size, mask_size, dtype=bool)\n",
    "        mask = torch.triu(mask).t().float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        mask = mask.masked_fill(mask == 1, 0.0)\n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_output_for_last_word(full_output: torch.Tensor) -> torch.Tensor:\n",
    "        return full_output[:,-1,:]\n",
    "    \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        mask_size = input.shape[0]\n",
    "        if self.input_mask is None or self.input_mask.size(0) != mask_size:\n",
    "            self.input_mask = self.__generate_input_mask(mask_size).to(device)\n",
    "            \n",
    "        input = self.embedding(input) * self.embedding_scale\n",
    "        input = self.positional_encoder(input)\n",
    "        output = self.transformer_encoder(input, self.input_mask)\n",
    "        #output = self.__get_output_for_last_word(output)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and batch data\n",
    "-------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
    "vocab object is built based on the train dataset and is used to numericalize\n",
    "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
    "function arranges the dataset into columns, trimming off any tokens remaining\n",
    "after the data has been divided into batches of size ``batch_size``.\n",
    "For instance, with the alphabet as the sequence (total length of 26)\n",
    "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
    "length 6:\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "  \\end{bmatrix}\\end{align}\n",
    "\n",
    "These columns are treated as independent by the model, which means that\n",
    "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
    "efficient batch processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104335, 20])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to generate input and target sequence\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``get_batch()`` function generates the input and target sequence for\n",
    "the transformer model. It subdivides the source data into chunks of\n",
    "length ``bptt``. For the language modeling task, the model needs the\n",
    "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "we’d get the following two Variables for ``i`` = 0:\n",
    "\n",
    "![](../_static/img/transformer_input_target.png)\n",
    "\n",
    "\n",
    "It should be noted that the chunks are along dimension 0, consistent\n",
    "with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "``N`` is along dimension 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence_length = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_parallel_data_streams(data, stream_count: int) -> torch.Tensor:\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    stream_length = data.size(0) // stream_count\n",
    "    data = data.narrow(0, 0, stream_length * stream_count)\n",
    "    data = data.view(stream_count, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def batch_loader(source, max_sequence_length: int) -> (torch.Tensor, torch.Tensor):\n",
    "    total_row_count = source.size(0)\n",
    "    # -1 to account for the target sequence shift\n",
    "    full_batch_count = (total_row_count - 1) // max_sequence_length\n",
    "    for batch_index in range(full_batch_count):\n",
    "        first_row_index = batch_index * max_sequence_length\n",
    "        last_row_index = first_row_index + max_sequence_length\n",
    "        inputs = source[first_row_index: last_row_index]\n",
    "        targets = source[first_row_index+1: last_row_index+1].view(-1)\n",
    "        yield inputs, targets\n",
    "        \n",
    "    first_row_index = full_batch_count * max_sequence_length\n",
    "    inputs = source[first_row_index:-1]\n",
    "    targets = source[first_row_index+1:].view(-1)\n",
    "    yield inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.eq(get_batch(train_data, 0)[0], next(batch_loader(train_data, 35))[0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an instance\n",
    "--------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is set up with the hyperparameter below. The vocab size is\n",
    "equal to the length of the vocab object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 3 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerScriptGenerator(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 3 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerScriptGenerator(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "| end of epoch  10 | time: 211.53s | valid loss  5.36 | valid ppl   212.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model\n",
    "-------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CrossEntropyLoss <https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
    "is applied to track the loss and\n",
    "`SGD <https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD>`__\n",
    "implements stochastic gradient descent method as the optimizer. The initial\n",
    "learning rate is set to 5.0. `StepLR <https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR>`__ is\n",
    "applied to adjust the learn rate through epochs. During the\n",
    "training, we use\n",
    "`nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_>`__\n",
    "function to scale all the gradient together to prevent exploding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104335, 20])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    \n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    \n",
    "    for batch_index, data_row_index in enumerate(range(0, train_data.size(0) - 1, input_sequence_length)):\n",
    "        print(batch_index, token_index)\n",
    "        data, targets = get_batch(train_data, data_row_index)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch_index % log_interval == 0 and batch_index > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch_index, len(train_data) // input_sequence_length, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, input_sequence_length):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(enumerate(batch_loader(train_data, input_sequence_length)))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2():\n",
    "    \n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    \n",
    "    for batch_index, (data, targets)  in enumerate(batch_loader(train_data, input_sequence_length)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch_index % log_interval == 0 and batch_index > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch_index, len(train_data) // input_sequence_length, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over epochs. Save the model if the validation loss is the best\n",
    "we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 62.87 | loss  6.81 | ppl   908.45\n",
      "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 62.93 | loss  6.50 | ppl   665.18\n",
      "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 64.94 | loss  6.23 | ppl   505.45\n",
      "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 67.67 | loss  6.15 | ppl   468.59\n",
      "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 63.86 | loss  6.05 | ppl   425.74\n",
      "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 63.21 | loss  6.04 | ppl   421.88\n",
      "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 63.28 | loss  6.00 | ppl   404.00\n",
      "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 63.65 | loss  6.02 | ppl   409.95\n",
      "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 63.00 | loss  5.92 | ppl   373.66\n",
      "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 62.83 | loss  5.94 | ppl   380.89\n",
      "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 62.92 | loss  5.82 | ppl   336.97\n",
      "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 63.11 | loss  5.87 | ppl   354.95\n",
      "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 64.05 | loss  5.87 | ppl   355.68\n",
      "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 63.22 | loss  5.78 | ppl   323.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 199.11s | valid loss  5.71 | valid ppl   302.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 63.63 | loss  5.74 | ppl   312.10\n",
      "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 62.94 | loss  5.74 | ppl   310.03\n",
      "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 63.31 | loss  5.58 | ppl   265.00\n",
      "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 62.78 | loss  5.62 | ppl   276.87\n",
      "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 63.10 | loss  5.57 | ppl   262.57\n",
      "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 62.76 | loss  5.61 | ppl   272.19\n",
      "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 62.85 | loss  5.61 | ppl   273.27\n",
      "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 63.13 | loss  5.65 | ppl   284.97\n",
      "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 63.14 | loss  5.57 | ppl   263.06\n",
      "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 62.72 | loss  5.61 | ppl   273.34\n",
      "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 61.42 | loss  5.49 | ppl   242.04\n",
      "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 61.63 | loss  5.56 | ppl   260.78\n",
      "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 61.77 | loss  5.57 | ppl   262.32\n",
      "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 61.43 | loss  5.49 | ppl   243.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 195.67s | valid loss  5.53 | valid ppl   252.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 63.74 | loss  5.50 | ppl   243.92\n",
      "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 62.80 | loss  5.52 | ppl   250.81\n",
      "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 62.96 | loss  5.34 | ppl   208.81\n",
      "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 63.02 | loss  5.40 | ppl   221.45\n",
      "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 62.23 | loss  5.36 | ppl   212.67\n",
      "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 61.84 | loss  5.40 | ppl   220.64\n",
      "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 61.61 | loss  5.41 | ppl   224.65\n",
      "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 61.49 | loss  5.46 | ppl   235.55\n",
      "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 63.39 | loss  5.39 | ppl   219.80\n",
      "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 64.53 | loss  5.41 | ppl   224.58\n",
      "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 64.10 | loss  5.30 | ppl   199.82\n",
      "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 63.70 | loss  5.38 | ppl   217.64\n",
      "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 63.37 | loss  5.39 | ppl   219.09\n",
      "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 63.85 | loss  5.33 | ppl   205.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 197.16s | valid loss  5.49 | valid ppl   241.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2981 batches | lr 4.07 | ms/batch 64.25 | loss  5.34 | ppl   207.91\n",
      "| epoch   4 |   400/ 2981 batches | lr 4.07 | ms/batch 64.32 | loss  5.37 | ppl   215.24\n",
      "| epoch   4 |   600/ 2981 batches | lr 4.07 | ms/batch 63.19 | loss  5.19 | ppl   179.00\n",
      "| epoch   4 |   800/ 2981 batches | lr 4.07 | ms/batch 63.08 | loss  5.25 | ppl   190.66\n",
      "| epoch   4 |  1000/ 2981 batches | lr 4.07 | ms/batch 63.52 | loss  5.21 | ppl   183.79\n",
      "| epoch   4 |  1200/ 2981 batches | lr 4.07 | ms/batch 66.49 | loss  5.25 | ppl   190.89\n",
      "| epoch   4 |  1400/ 2981 batches | lr 4.07 | ms/batch 66.92 | loss  5.28 | ppl   196.42\n",
      "| epoch   4 |  1600/ 2981 batches | lr 4.07 | ms/batch 66.00 | loss  5.33 | ppl   205.78\n",
      "| epoch   4 |  1800/ 2981 batches | lr 4.07 | ms/batch 65.92 | loss  5.27 | ppl   193.89\n",
      "| epoch   4 |  2000/ 2981 batches | lr 4.07 | ms/batch 65.73 | loss  5.29 | ppl   197.48\n",
      "| epoch   4 |  2200/ 2981 batches | lr 4.07 | ms/batch 65.85 | loss  5.17 | ppl   175.43\n",
      "| epoch   4 |  2400/ 2981 batches | lr 4.07 | ms/batch 65.82 | loss  5.25 | ppl   190.01\n",
      "| epoch   4 |  2600/ 2981 batches | lr 4.07 | ms/batch 66.07 | loss  5.26 | ppl   193.28\n",
      "| epoch   4 |  2800/ 2981 batches | lr 4.07 | ms/batch 65.62 | loss  5.20 | ppl   180.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 204.45s | valid loss  5.42 | valid ppl   225.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2981 batches | lr 3.87 | ms/batch 66.35 | loss  5.23 | ppl   186.02\n",
      "| epoch   5 |   400/ 2981 batches | lr 3.87 | ms/batch 65.99 | loss  5.26 | ppl   191.93\n",
      "| epoch   5 |   600/ 2981 batches | lr 3.87 | ms/batch 65.70 | loss  5.06 | ppl   157.70\n",
      "| epoch   5 |   800/ 2981 batches | lr 3.87 | ms/batch 65.48 | loss  5.13 | ppl   168.71\n",
      "| epoch   5 |  1000/ 2981 batches | lr 3.87 | ms/batch 66.09 | loss  5.09 | ppl   163.20\n",
      "| epoch   5 |  1200/ 2981 batches | lr 3.87 | ms/batch 65.65 | loss  5.14 | ppl   170.59\n",
      "| epoch   5 |  1400/ 2981 batches | lr 3.87 | ms/batch 65.96 | loss  5.17 | ppl   175.70\n",
      "| epoch   5 |  1600/ 2981 batches | lr 3.87 | ms/batch 65.60 | loss  5.22 | ppl   184.51\n",
      "| epoch   5 |  1800/ 2981 batches | lr 3.87 | ms/batch 66.08 | loss  5.16 | ppl   173.86\n",
      "| epoch   5 |  2000/ 2981 batches | lr 3.87 | ms/batch 66.05 | loss  5.17 | ppl   176.69\n",
      "| epoch   5 |  2200/ 2981 batches | lr 3.87 | ms/batch 65.87 | loss  5.05 | ppl   155.79\n",
      "| epoch   5 |  2400/ 2981 batches | lr 3.87 | ms/batch 65.63 | loss  5.14 | ppl   170.62\n",
      "| epoch   5 |  2600/ 2981 batches | lr 3.87 | ms/batch 65.75 | loss  5.16 | ppl   173.37\n",
      "| epoch   5 |  2800/ 2981 batches | lr 3.87 | ms/batch 65.79 | loss  5.09 | ppl   162.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 206.29s | valid loss  5.42 | valid ppl   225.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2981 batches | lr 3.68 | ms/batch 66.18 | loss  5.12 | ppl   168.04\n",
      "| epoch   6 |   400/ 2981 batches | lr 3.68 | ms/batch 65.79 | loss  5.16 | ppl   173.63\n",
      "| epoch   6 |   600/ 2981 batches | lr 3.68 | ms/batch 65.87 | loss  4.96 | ppl   142.94\n",
      "| epoch   6 |   800/ 2981 batches | lr 3.68 | ms/batch 65.99 | loss  5.04 | ppl   153.86\n",
      "| epoch   6 |  1000/ 2981 batches | lr 3.68 | ms/batch 66.16 | loss  5.01 | ppl   149.43\n",
      "| epoch   6 |  1200/ 2981 batches | lr 3.68 | ms/batch 64.34 | loss  5.04 | ppl   155.09\n",
      "| epoch   6 |  1400/ 2981 batches | lr 3.68 | ms/batch 62.92 | loss  5.08 | ppl   160.88\n",
      "| epoch   6 |  1600/ 2981 batches | lr 3.68 | ms/batch 62.47 | loss  5.14 | ppl   170.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |  1800/ 2981 batches | lr 3.68 | ms/batch 62.90 | loss  5.07 | ppl   158.81\n",
      "| epoch   6 |  2000/ 2981 batches | lr 3.68 | ms/batch 62.82 | loss  5.08 | ppl   161.49\n",
      "| epoch   6 |  2200/ 2981 batches | lr 3.68 | ms/batch 62.83 | loss  4.96 | ppl   142.04\n",
      "| epoch   6 |  2400/ 2981 batches | lr 3.68 | ms/batch 62.56 | loss  5.04 | ppl   154.33\n",
      "| epoch   6 |  2600/ 2981 batches | lr 3.68 | ms/batch 63.31 | loss  5.07 | ppl   158.78\n",
      "| epoch   6 |  2800/ 2981 batches | lr 3.68 | ms/batch 63.99 | loss  5.00 | ppl   148.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 201.89s | valid loss  5.44 | valid ppl   229.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2981 batches | lr 3.49 | ms/batch 66.06 | loss  5.03 | ppl   153.66\n",
      "| epoch   7 |   400/ 2981 batches | lr 3.49 | ms/batch 66.38 | loss  5.07 | ppl   158.78\n",
      "| epoch   7 |   600/ 2981 batches | lr 3.49 | ms/batch 66.50 | loss  4.87 | ppl   129.83\n",
      "| epoch   7 |   800/ 2981 batches | lr 3.49 | ms/batch 66.66 | loss  4.95 | ppl   140.64\n",
      "| epoch   7 |  1000/ 2981 batches | lr 3.49 | ms/batch 66.13 | loss  4.92 | ppl   137.16\n",
      "| epoch   7 |  1200/ 2981 batches | lr 3.49 | ms/batch 66.58 | loss  4.96 | ppl   142.09\n",
      "| epoch   7 |  1400/ 2981 batches | lr 3.49 | ms/batch 66.32 | loss  4.98 | ppl   146.08\n",
      "| epoch   7 |  1600/ 2981 batches | lr 3.49 | ms/batch 66.05 | loss  5.04 | ppl   154.64\n",
      "| epoch   7 |  1800/ 2981 batches | lr 3.49 | ms/batch 66.37 | loss  4.99 | ppl   146.32\n",
      "| epoch   7 |  2000/ 2981 batches | lr 3.49 | ms/batch 66.30 | loss  5.01 | ppl   149.40\n",
      "| epoch   7 |  2200/ 2981 batches | lr 3.49 | ms/batch 66.11 | loss  4.87 | ppl   130.18\n",
      "| epoch   7 |  2400/ 2981 batches | lr 3.49 | ms/batch 66.25 | loss  4.96 | ppl   142.35\n",
      "| epoch   7 |  2600/ 2981 batches | lr 3.49 | ms/batch 66.62 | loss  4.98 | ppl   145.36\n",
      "| epoch   7 |  2800/ 2981 batches | lr 3.49 | ms/batch 66.52 | loss  4.92 | ppl   136.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 208.01s | valid loss  5.37 | valid ppl   214.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2981 batches | lr 3.32 | ms/batch 66.68 | loss  4.96 | ppl   141.89\n",
      "| epoch   8 |   400/ 2981 batches | lr 3.32 | ms/batch 66.30 | loss  4.99 | ppl   146.52\n",
      "| epoch   8 |   600/ 2981 batches | lr 3.32 | ms/batch 65.76 | loss  4.79 | ppl   120.55\n",
      "| epoch   8 |   800/ 2981 batches | lr 3.32 | ms/batch 65.98 | loss  4.87 | ppl   130.19\n",
      "| epoch   8 |  1000/ 2981 batches | lr 3.32 | ms/batch 66.21 | loss  4.84 | ppl   126.72\n",
      "| epoch   8 |  1200/ 2981 batches | lr 3.32 | ms/batch 66.38 | loss  4.88 | ppl   132.21\n",
      "| epoch   8 |  1400/ 2981 batches | lr 3.32 | ms/batch 66.40 | loss  4.91 | ppl   135.87\n",
      "| epoch   8 |  1600/ 2981 batches | lr 3.32 | ms/batch 64.58 | loss  4.97 | ppl   143.68\n",
      "| epoch   8 |  1800/ 2981 batches | lr 3.32 | ms/batch 63.60 | loss  4.90 | ppl   134.58\n",
      "| epoch   8 |  2000/ 2981 batches | lr 3.32 | ms/batch 63.36 | loss  4.92 | ppl   137.18\n",
      "| epoch   8 |  2200/ 2981 batches | lr 3.32 | ms/batch 63.16 | loss  4.79 | ppl   120.73\n",
      "| epoch   8 |  2400/ 2981 batches | lr 3.32 | ms/batch 64.53 | loss  4.87 | ppl   130.95\n",
      "| epoch   8 |  2600/ 2981 batches | lr 3.32 | ms/batch 65.99 | loss  4.90 | ppl   134.11\n",
      "| epoch   8 |  2800/ 2981 batches | lr 3.32 | ms/batch 66.84 | loss  4.85 | ppl   127.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 205.36s | valid loss  5.35 | valid ppl   209.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2981 batches | lr 3.15 | ms/batch 68.55 | loss  4.88 | ppl   132.00\n",
      "| epoch   9 |   400/ 2981 batches | lr 3.15 | ms/batch 68.19 | loss  4.91 | ppl   135.70\n",
      "| epoch   9 |   600/ 2981 batches | lr 3.15 | ms/batch 68.25 | loss  4.72 | ppl   111.70\n",
      "| epoch   9 |   800/ 2981 batches | lr 3.15 | ms/batch 68.20 | loss  4.79 | ppl   120.71\n",
      "| epoch   9 |  1000/ 2981 batches | lr 3.15 | ms/batch 68.21 | loss  4.78 | ppl   118.64\n",
      "| epoch   9 |  1200/ 2981 batches | lr 3.15 | ms/batch 68.47 | loss  4.81 | ppl   122.89\n",
      "| epoch   9 |  1400/ 2981 batches | lr 3.15 | ms/batch 68.51 | loss  4.84 | ppl   126.05\n",
      "| epoch   9 |  1600/ 2981 batches | lr 3.15 | ms/batch 68.26 | loss  4.90 | ppl   134.01\n",
      "| epoch   9 |  1800/ 2981 batches | lr 3.15 | ms/batch 68.38 | loss  4.84 | ppl   126.40\n",
      "| epoch   9 |  2000/ 2981 batches | lr 3.15 | ms/batch 68.09 | loss  4.85 | ppl   128.25\n",
      "| epoch   9 |  2200/ 2981 batches | lr 3.15 | ms/batch 68.52 | loss  4.72 | ppl   112.32\n",
      "| epoch   9 |  2400/ 2981 batches | lr 3.15 | ms/batch 68.28 | loss  4.80 | ppl   121.83\n",
      "| epoch   9 |  2600/ 2981 batches | lr 3.15 | ms/batch 68.28 | loss  4.83 | ppl   124.62\n",
      "| epoch   9 |  2800/ 2981 batches | lr 3.15 | ms/batch 68.17 | loss  4.78 | ppl   118.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 213.76s | valid loss  5.32 | valid ppl   204.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2981 batches | lr 2.99 | ms/batch 68.91 | loss  4.82 | ppl   123.54\n",
      "| epoch  10 |   400/ 2981 batches | lr 2.99 | ms/batch 68.34 | loss  4.84 | ppl   126.40\n",
      "| epoch  10 |   600/ 2981 batches | lr 2.99 | ms/batch 68.29 | loss  4.66 | ppl   105.34\n",
      "| epoch  10 |   800/ 2981 batches | lr 2.99 | ms/batch 68.34 | loss  4.73 | ppl   113.09\n",
      "| epoch  10 |  1000/ 2981 batches | lr 2.99 | ms/batch 68.21 | loss  4.71 | ppl   110.52\n",
      "| epoch  10 |  1200/ 2981 batches | lr 2.99 | ms/batch 68.32 | loss  4.75 | ppl   115.73\n",
      "| epoch  10 |  1400/ 2981 batches | lr 2.99 | ms/batch 68.13 | loss  4.78 | ppl   118.55\n",
      "| epoch  10 |  1600/ 2981 batches | lr 2.99 | ms/batch 68.15 | loss  4.83 | ppl   125.72\n",
      "| epoch  10 |  1800/ 2981 batches | lr 2.99 | ms/batch 68.41 | loss  4.78 | ppl   118.55\n",
      "| epoch  10 |  2000/ 2981 batches | lr 2.99 | ms/batch 68.40 | loss  4.80 | ppl   120.93\n",
      "| epoch  10 |  2200/ 2981 batches | lr 2.99 | ms/batch 67.80 | loss  4.65 | ppl   104.83\n",
      "| epoch  10 |  2400/ 2981 batches | lr 2.99 | ms/batch 64.70 | loss  4.74 | ppl   114.35\n",
      "| epoch  10 |  2600/ 2981 batches | lr 2.99 | ms/batch 64.93 | loss  4.76 | ppl   117.14\n",
      "| epoch  10 |  2800/ 2981 batches | lr 2.99 | ms/batch 64.91 | loss  4.71 | ppl   110.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 210.20s | valid loss  5.36 | valid ppl   213.06\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 10 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train2()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with the test dataset\n",
    "-------------------------------------\n",
    "\n",
    "Apply the best model to check the result with the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.27 | test ppl   195.25\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28785\n",
      "BPTT = 35\n",
      "Input shape = torch.Size([35, 20])\n",
      "Target shape = torch.Size([700])\n",
      "Output shape = torch.Size([35, 20, 28785])\n",
      "Output flat shape = torch.Size([700, 28785])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 9.6921, -0.7327, -0.7593,  ...,  0.0781,  0.1744, -1.0999],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.eval();\n",
    "vocab_size = len(TEXT.vocab.stoi)\n",
    "print(vocab_size)\n",
    "data, targets = get_batch(train_data, 0)\n",
    "print(f\"BPTT = {bptt}\")\n",
    "print(f\"Input shape = {data.shape}\")\n",
    "print(f\"Target shape = {targets.shape}\")\n",
    "output = best_model(data)\n",
    "print(f\"Output shape = {output.shape}\")\n",
    "output_flat = output.view(-1, ntokens)\n",
    "print(f\"Output flat shape = {output_flat.shape}\")\n",
    "output_flat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_list(tensor: torch.Tensor) -> list:\n",
    "    return tensor.cpu().detach().numpy().tolist()\n",
    "\n",
    "\n",
    "def scores_to_top_tokens(scores: torch.Tensor) -> list:\n",
    "    _, top_index_tensor = torch.topk(scores, k=1)\n",
    "    top_index_tensor.squeeze_().t_()\n",
    "    top_index_list = tensor_to_list(top_index_tensor)\n",
    "    return top_index_list\n",
    "\n",
    "\n",
    "def tokens_to_words(batches: list) -> list:\n",
    "    decoded_batches = []\n",
    "    for batch in batches:\n",
    "        decoded_batch = [TEXT.vocab.itos[token] for token in batch]\n",
    "        decoded_batches.append(decoded_batch)\n",
    "    return decoded_batches\n",
    "\n",
    "\n",
    "def decode_transformer_output(scores: torch.Tensor) -> list:\n",
    "    top_tokens = scores_to_top_tokens(scores)\n",
    "    predicted_words = tokens_to_words(top_tokens)\n",
    "    return predicted_words\n",
    "\n",
    "\n",
    "def decode_targets(targets: torch.Tensor, batch_size: int) -> list:\n",
    "    unsquashed_targets = targets.view(-1, batch_size).t()\n",
    "    target_tokens = tensor_to_list(unsquashed_targets)\n",
    "    return tokens_to_words(target_tokens)\n",
    "\n",
    "\n",
    "def decode_inputs(inputs: torch.Tensor) -> list:\n",
    "    input_tokens = tensor_to_list(inputs.t())\n",
    "    return tokens_to_words(input_tokens)\n",
    "\n",
    "\n",
    "def join_sequences(decoded_sequences: list) -> list:\n",
    "    return [\" \".join(sequence) for sequence in decoded_sequences]\n",
    "\n",
    "\n",
    "def present_result(model, inputs: torch.Tensor, targets: torch.Tensor, index: int) -> None:\n",
    "    model.eval()\n",
    "    output = model(inputs)\n",
    "    input_text = join_sequences(decode_inputs(inputs))[index]\n",
    "    target_text = join_sequences(decode_targets(targets, 20))[index]\n",
    "    output_text = join_sequences(decode_transformer_output(output))[index]\n",
    "    print( \"INPUT\\n-----\\n\" \n",
    "          f\"{input_text}\\n\\n\"\n",
    "           \"TARGET\\n------\\n\"\n",
    "          f\"{target_text}\\n\\n\"\n",
    "           \"OUTPUT\\n------\\n\"\n",
    "          f\"{output_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT\n",
      "-----\n",
      ". <unk> sharif , the cambridge <unk> , became the tenth female to steer a boat race crew , and was the lightest competitor at the event since the 1986 race . <eos> <eos> =\n",
      "\n",
      "TARGET\n",
      "------\n",
      "<unk> sharif , the cambridge <unk> , became the tenth female to steer a boat race crew , and was the lightest competitor at the event since the 1986 race . <eos> <eos> = =\n",
      "\n",
      "OUTPUT\n",
      "------\n",
      "<eos> , , <unk> <unk> , , <unk> the first of protagonist the the <unk> race , , and <unk> forced second , , the end , the end season , <eos> <eos> = =\n",
      "\n"
     ]
    }
   ],
   "source": [
    "present_result(best_model, data, targets, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(text: str) -> str:\n",
    "    encoded_text = TEXT.numericalize([text.split()]).to(device)\n",
    "    custom_out = best_model(encoded_text)\n",
    "    custom_out.shape\n",
    "    _, custom_top_tokens = torch.topk(custom_out, k=1)\n",
    "    out_tokens = tensor_to_list(custom_top_tokens.squeeze())\n",
    "    return [TEXT.vocab.itos[token] for token in out_tokens][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that the primary schools , and the kakapo\n",
      "that the primary schools , and the kakapo is\n",
      "that the primary schools , and the kakapo is the\n",
      "that the primary schools , and the kakapo is the largest\n",
      "that the primary schools , and the kakapo is the largest <unk>\n",
      "that the primary schools , and the kakapo is the largest <unk> ,\n",
      "that the primary schools , and the kakapo is the largest <unk> , and\n",
      "that the primary schools , and the kakapo is the largest <unk> , and the\n",
      "that the primary schools , and the kakapo is the largest <unk> , and the <unk>\n",
      "that the primary schools , and the kakapo is the largest <unk> , and the <unk> ,\n",
      "that the primary schools , and the kakapo is the largest <unk> , and the <unk> , and\n",
      "that the primary schools , and the kakapo is the largest <unk> , and the <unk> , and the\n",
      "that the primary schools , and the kakapo is the largest <unk> , and the <unk> , and the <unk>\n",
      "the primary schools , and the kakapo is the largest <unk> , and the <unk> , and the <unk> ,\n",
      "primary schools , and the kakapo is the largest <unk> , and the <unk> , and the <unk> , <unk>\n",
      "schools , and the kakapo is the largest <unk> , and the <unk> , and the <unk> , <unk> ,\n",
      ", and the kakapo is the largest <unk> , and the <unk> , and the <unk> , <unk> , <unk>\n",
      "and the kakapo is the largest <unk> , and the <unk> , and the <unk> , <unk> , <unk> ,\n",
      "the kakapo is the largest <unk> , and the <unk> , and the <unk> , <unk> , <unk> , <unk>\n",
      "kakapo is the largest <unk> , and the <unk> , and the <unk> , <unk> , <unk> , <unk> ,\n",
      "is the largest <unk> , and the <unk> , and the <unk> , <unk> , <unk> , <unk> , <unk>\n",
      "the largest <unk> , and the <unk> , and the <unk> , <unk> , <unk> , <unk> , <unk> ,\n",
      "largest <unk> , and the <unk> , and the <unk> , <unk> , <unk> , <unk> , <unk> , <unk>\n",
      "<unk> , and the <unk> , and the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> ,\n",
      ", and the <unk> , and the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk>\n",
      "and the <unk> , and the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> ,\n",
      "the <unk> , and the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk>\n",
      "<unk> , and the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> ,\n",
      ", and the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk>\n",
      "and the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> ,\n",
      "that the primary schools , and the kakapo is the largest <unk> , and the <unk> , and the <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> \n"
     ]
    }
   ],
   "source": [
    "sentence = \"that the primary schools , and the kakapo \"\n",
    "for i in range(30):\n",
    "    input = \" \".join(sentence.split()[-20:])\n",
    "    print(input)\n",
    "    sentence += predict_next_word(input) + \" \"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, input_sequence_length):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
